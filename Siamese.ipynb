{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pillow\n",
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "\n",
    "# # using dataset https://www.kaggle.com/datasets/stoicstatic/face-recognition-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49309d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from keras.applications.mobilenet  import MobileNet\n",
    "from keras.applications.mobilenet  import preprocess_input\n",
    "\n",
    "\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "tf.random.set_seed(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aaa679",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_FACE_PATH = './ExtractedFaces'\n",
    "\n",
    "def readImage(index):\n",
    "    path = os.path.join(FOLDER_FACE_PATH, index[0], index[1])\n",
    "    img = cv2.imread(path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def splitDataset(ratio=0.8):\n",
    "    train = {}\n",
    "    test = {}\n",
    "    folders = os.listdir(FOLDER_FACE_PATH)\n",
    "    folderTrainSize = int(len(folders) * ratio)\n",
    "    random.shuffle(folders)\n",
    "\n",
    "    trainFolders = folders[:folderTrainSize]\n",
    "    testFolders = folders[folderTrainSize:]\n",
    "    for folder in trainFolders:\n",
    "        files = os.listdir(os.path.join(FOLDER_FACE_PATH, folder))\n",
    "        train[folder] = files\n",
    "\n",
    "    for folder in testFolders:\n",
    "        files = os.listdir(os.path.join(FOLDER_FACE_PATH, folder))\n",
    "        test[folder] = files\n",
    "            \n",
    "    return train, test\n",
    "train, test = splitDataset(0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cfa1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createTriplet(train):\n",
    "    # anchor and positive are from same class, negative is from different class\n",
    "    triplets = []\n",
    "    folders = list(train.keys())\n",
    "\n",
    "    for folder in folders:\n",
    "        files = train[folder]\n",
    "        if len(files) < 2:\n",
    "            continue\n",
    "        anchorPositivePairs = []\n",
    "        for i in range(len(files)):\n",
    "            for j in range(i + 1, len(files)):\n",
    "                anchorPositivePairs.append((files[i], files[j]))\n",
    "\n",
    "        for anchor, positive in anchorPositivePairs:\n",
    "            negativeFolder = folder\n",
    "            while negativeFolder == folder:\n",
    "                negativeFolder = random.choice(folders)\n",
    "            negativeFiles = train[negativeFolder]\n",
    "            negative = random.choice(negativeFiles)\n",
    "            triplets.append(((folder, anchor), (folder, positive), (negativeFolder, negative)))\n",
    "            \n",
    "    random.shuffle(triplets)\n",
    "    return triplets\n",
    "\n",
    "tripletsTrain = createTriplet(train)\n",
    "tripletsTest = createTriplet(test)\n",
    "print (len(tripletsTrain), 'triplets created')\n",
    "print (len(tripletsTest), 'total triplets created')\n",
    "\n",
    "# Show image triplet\n",
    "for i in range(3):\n",
    "    print(tripletsTrain[i])\n",
    "    anchor = readImage(tripletsTrain[i][0])\n",
    "    positive = readImage(tripletsTrain[i][1])\n",
    "    negative = readImage(tripletsTrain[i][2])\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(anchor)\n",
    "    plt.title('Anchor')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(positive)\n",
    "    plt.title('Positive')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(negative)\n",
    "    plt.title('Negative')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGenerator(triplets, batchSize=32):\n",
    "    while True:\n",
    "        anchorBatch = []\n",
    "        positiveBatch = []\n",
    "        negativeBatch = []\n",
    "        for i in range(batchSize):\n",
    "            triplet = random.choice(triplets)\n",
    "            anchor = readImage(triplet[0])\n",
    "            positive = readImage(triplet[1])\n",
    "            negative = readImage(triplet[2])\n",
    "\n",
    "            anchor = preprocess_input(anchor)\n",
    "            positive = preprocess_input(positive)\n",
    "            negative = preprocess_input(negative)\n",
    "\n",
    "            anchorBatch.append(anchor)\n",
    "            positiveBatch.append(positive)\n",
    "            negativeBatch.append(negative)\n",
    "\n",
    "        anchorBatch = tf.convert_to_tensor(np.array(anchorBatch, dtype=\"float32\"))\n",
    "        positiveBatch = tf.convert_to_tensor(np.array(positiveBatch, dtype=\"float32\"))\n",
    "        negativeBatch = tf.convert_to_tensor(np.array(negativeBatch, dtype=\"float32\"))\n",
    "        labels = tf.zeros((batchSize, 1), dtype=tf.float32)\n",
    "\n",
    "        yield (anchorBatch, positiveBatch, negativeBatch), labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ebe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingModel(inputShape):\n",
    "    baseModel = MobileNet(input_shape=inputShape, include_top=False, weights='imagenet', pooling='avg')\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        baseModel,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
    "    ], name='Embedding')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create siamese network model with triplet loss\n",
    "\n",
    "# Distance layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
    "class DistanceLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "    \n",
    "\n",
    "def getSiameseModel(inputShape=(128, 128, 3)):\n",
    "    embeddingModel = getEmbeddingModel(inputShape)\n",
    "\n",
    "    anchorInput = tf.keras.layers.Input(name='anchor', shape=inputShape)\n",
    "    positiveInput = tf.keras.layers.Input(name='positive', shape=inputShape)\n",
    "    negativeInput = tf.keras.layers.Input(name='negative', shape=inputShape)\n",
    "\n",
    "    anchorEmbedding = embeddingModel(anchorInput)\n",
    "    positiveEmbedding = embeddingModel(positiveInput)\n",
    "    negativeEmbedding = embeddingModel(negativeInput)\n",
    "\n",
    "    # Distance layer\n",
    "    distances = DistanceLayer()(anchorEmbedding, positiveEmbedding, negativeEmbedding)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[anchorInput, positiveInput, negativeInput], outputs=distances, name='SiameseNetwork')\n",
    "    return model\n",
    "\n",
    "SiameseModel  = getSiameseModel()\n",
    "SiameseModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training step with triplet loss\n",
    "class SiameseModelClass(tf.keras.Model):\n",
    "    def __init__(self, siameseModel, margin=0.5):\n",
    "        super(SiameseModelClass, self).__init__()\n",
    "        self.siameseModel = siameseModel\n",
    "        self.margin = margin\n",
    "        self.lossTracker = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siameseModel(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            ap_distance, an_distance = self.siameseModel(data[0], training=True)\n",
    "            loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.siameseModel.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.siameseModel.trainable_weights))\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {'loss': self.lossTracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        ap_distance, an_distance = self.siameseModel(data[0], training=False)\n",
    "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {'loss': self.lossTracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.lossTracker]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60195330",
   "metadata": {},
   "outputs": [],
   "source": [
    "siameseModel = SiameseModelClass(SiameseModel)\n",
    "siameseModel.compile(optimizer=tf.keras.optimizers.Adam(1e-4))\n",
    "\n",
    "# early stopping callback\n",
    "cb_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1e-4,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.5, \n",
    "                              patience=3, \n",
    "                              verbose=1, \n",
    "                              mode='min', \n",
    "                              min_lr=0.00001)\n",
    "\n",
    "cb_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'checkpoints/siameseModel.keras',\n",
    "    monitor= 'val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "history = siameseModel.fit(batchGenerator(tripletsTrain, batchSize=128),\n",
    "          steps_per_epoch=100,\n",
    "          validation_data=batchGenerator(tripletsTest, batchSize=128),\n",
    "          callbacks=[cb_early_stopping, cb_model_checkpoint,reduce_lr],\n",
    "          validation_steps=100,\n",
    "          epochs=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model \n",
    "\n",
    "def extractEncoder(model):\n",
    "    encoder = getEmbeddingModel((128, 128, 3))\n",
    "    i=0\n",
    "    for e_layer in model.layers[0].layers[3].layers:\n",
    "        layer_weight = e_layer.get_weights()\n",
    "        encoder.layers[i].set_weights(layer_weight)\n",
    "        i+=1\n",
    "    return encoder\n",
    "\n",
    "encoder = extractEncoder(siameseModel)\n",
    "encoder.save_weights(\"encoder.weights.h5\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(face_list1, face_list2, threshold=1.1):\n",
    "    # Getting the encodings for the passed faces\n",
    "    tensor1 = encoder.predict(face_list1)\n",
    "    tensor2 = encoder.predict(face_list2)\n",
    "    \n",
    "    # Euclidean distance between the encodings\n",
    "    distance = np.sqrt(np.sum(np.square(tensor1-tensor2), axis=-1))\n",
    "    prediction = np.where(distance<=threshold, 0, 1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = np.array([])\n",
    "neg_list = np.array([])\n",
    "\n",
    "for data in batchGenerator(tripletsTest, batchSize=256):\n",
    "    print(data)\n",
    "    a, p, n = data[0]\n",
    "    pos_list = np.append(pos_list, classify_images(a, p))\n",
    "    neg_list = np.append(neg_list, classify_images(a, n))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ModelMetrics(pos_list, neg_list):\n",
    "    true = np.array([0]*len(pos_list)+[1]*len(neg_list))\n",
    "    pred = np.append(pos_list, neg_list)\n",
    "    \n",
    "    # Compute and print the accuracy\n",
    "    print(f\"\\nAccuracy of model: {accuracy_score(true, pred)}\\n\")\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(true, pred)\n",
    "\n",
    "    categories  = ['Similar','Different']\n",
    "    names = ['True Similar','False Similar', 'False Different','True Different']\n",
    "    percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
    "\n",
    "    # Accuracy, Precision, Recall, F1-Score\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(true, pred, target_names=categories))\n",
    "ModelMetrics(pos_list, neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec05711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for 2 images\n",
    "face_1 = readImage(('bin', '0.jpg'))\n",
    "face_2 = readImage(('bin', '1.jpg'))\n",
    "\n",
    "\n",
    "face_1 = cv2.resize(face_1, (128, 128))\n",
    "face_2 = cv2.resize(face_2, (128, 128))\n",
    "\n",
    "# show images\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(face_1)\n",
    "plt.title('Face 1')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(face_2)\n",
    "plt.title('Face 2')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "face_1 = preprocess_input(face_1)\n",
    "face_2 = preprocess_input(face_2)\n",
    "\n",
    "#\n",
    "face_1 = tf.convert_to_tensor(np.array([face_1], dtype=\"float32\"))\n",
    "face_2 = tf.convert_to_tensor(np.array([face_2], dtype=\"float32\"))\n",
    "\n",
    "prediction = classify_images(face_1, face_2, threshold=1.0)\n",
    "print('Same person' if prediction[0]==0 else 'Different person')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
